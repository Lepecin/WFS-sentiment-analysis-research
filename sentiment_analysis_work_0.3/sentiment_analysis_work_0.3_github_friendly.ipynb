{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential packages\n",
    "\n",
    "# Import praw, the Python Reddit API Wrapper\n",
    "import praw\n",
    "\n",
    "# Import datetime, for simple date and time storing\n",
    "import datetime\n",
    "\n",
    "# Import pandas, for data storage\n",
    "import pandas as pd\n",
    "\n",
    "# Import matplotlib.pyplot, for creating plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy, for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# Extra packages\n",
    "\n",
    "# Import pprint\n",
    "# import pprint\n",
    "\n",
    "# Import seaborn\n",
    "# import seaborn as sns\n",
    "\n",
    "# Import psaw\n",
    "# import psaw\n",
    "\n",
    "# Import pandas data reader\n",
    "# import pandas_datareader as web\n",
    "\n",
    "# Import dateutil\n",
    "# from dateutil import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP packages\n",
    "\n",
    "# Import nltk\n",
    "import nltk\n",
    "\n",
    "# Download the Vader lexicon dataset for\n",
    "# sentiment analysis\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# Import the sentiment intensity analyser\n",
    "# function from nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "# Create the SIA as a simple referable\n",
    "# object\n",
    "sia = SIA()\n",
    "\n",
    "# Reference code for extracting sentiment\n",
    "# data\n",
    "\n",
    "#results = []\n",
    "#line = \"APLL is a nice stock\"\n",
    "#score = sia.polarity_scores(line)\n",
    "#results.append(score)\n",
    "#pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For installing psaw package\n",
    "# Done using installed pip package installer\n",
    "# pip install psaw\n",
    "\n",
    "# Set up a Read-Only Reddit client\n",
    "reddit = praw.Reddit(\n",
    "    \n",
    "    # Input the client id\n",
    "    client_id = \"<client_id>\",\n",
    "    \n",
    "    # Input the client secret\n",
    "    client_secret = \"<client_secret>\",\n",
    "    \n",
    "    # Set the name of the client\n",
    "    user_agent = \"<user_agent>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the re package\n",
    "import re\n",
    "\n",
    "# Import the collections package\n",
    "import collections\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Function for creating dictionary, counting mentions\n",
    "# of any stock tickers it can find\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def remove_redundant(txt):\n",
    "\n",
    "  # re function for striping all redundant characters \n",
    "  # in input txt\n",
    "  txt = re.sub('[^^/A-Z]', ' ', txt)\n",
    "\n",
    "  # Split the stripped text into chunks\n",
    "  txt = txt.split()\n",
    "\n",
    "  # Use Counter to find frequency of elements\n",
    "  frequency = collections.Counter(list(txt))\n",
    "\n",
    "  # Turn the frequency object into a dictionary\n",
    "  frequency = dict(frequency)\n",
    "\n",
    "  # Filter any entries in frequency that do not belong\n",
    "  # to the list of stock ticker symbols\n",
    "  filtered_frequency = {k: v for k, v in frequency.items() if k in st_tickers}\n",
    "\n",
    "  # Return the modified txt\n",
    "  return filtered_frequency\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Function for adding entries for two\n",
    "# dictionaries\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def dict_combine(dict_1, dict_2):\n",
    "\n",
    "    # Define the counter version of first dictionary\n",
    "    a_counter = collections.Counter(dict_1)\n",
    "\n",
    "    # Do same for next dictionary\n",
    "    b_counter = collections.Counter(dict_2)\n",
    "\n",
    "    # Define the output dictionary as the sum of\n",
    "    # counters of input dictionaries\n",
    "    dict_3 = dict(a_counter + b_counter)\n",
    "\n",
    "    # Output the combined dictionaries\n",
    "    return dict_3\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Data extraction function :)\n",
    "# The holy grail\n",
    "# Add text and id, then await magic to retrieve\n",
    "# your data :D\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def data_extract(comm, txt):\n",
    "    \n",
    "    # Convert the comment number into a dataset\n",
    "    comm_unitdata = pd.DataFrame(data = [comm], columns = ['comment'])\n",
    "\n",
    "    # Retrieve the ticker counts data from input\n",
    "    # text\n",
    "    txt_data = remove_redundant(txt).items()\n",
    "\n",
    "    # Convert the ticker counts data into\n",
    "    # dataframe object\n",
    "    txt_data = pd.DataFrame(data = txt_data, columns = ['ticker','count'])\n",
    "\n",
    "    # Concatenate the comm and txt_data datasets\n",
    "    comm_data = pd.concat([comm_unitdata ,txt_data], sort = False, axis = 1)\n",
    "\n",
    "    # Modify the comment column to be filled with\n",
    "    # the comment number\n",
    "    comm_data['comment'] = comm_data['comment'].fillna(comm)\n",
    "\n",
    "    # Set comm_data to be a none object is it\n",
    "    # has missing data\n",
    "    if comm_data.isnull().any().any():\n",
    "        comm_data = None\n",
    "\n",
    "    # Extract the sentiment intensity data from\n",
    "    # the text\n",
    "    score = sia.polarity_scores(txt)\n",
    "\n",
    "    # Record the sentiment intensity data into\n",
    "    # a data frame\n",
    "    sent_data = pd.DataFrame.from_records([score])\n",
    "\n",
    "    # Concatenate the sentiment dataset with the\n",
    "    # comm_unitdata dataset\n",
    "    sent_data = pd.concat([comm_unitdata ,sent_data], sort = False, axis = 1)\n",
    "\n",
    "    # Return the two datasets\n",
    "    return sent_data, comm_data\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Function for creating datasets for\n",
    "# ticker counts, comment dates, and\n",
    "# sentiment data\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def ticker_puller(submission):\n",
    "\n",
    "    # Create three empty datasets\n",
    "\n",
    "    # One for setniment intensity data\n",
    "    sentiment_intensity_data = pd.DataFrame(data = [])\n",
    "\n",
    "    # One for ticker counts data\n",
    "    ticker_counts_data = pd.DataFrame(data = [])\n",
    "\n",
    "    # Last for the dates and text data\n",
    "    comment_datetxt_data = pd.DataFrame(data = [])\n",
    "\n",
    "\n",
    "    # Start extracting data from the title of\n",
    "    # the submission, which will be comment zero\n",
    "\n",
    "    # Extract the title text\n",
    "    title_txt = submission.title\n",
    "\n",
    "    # Extract the ticker and sentiment data from\n",
    "    # the title text\n",
    "    title_vanilla_extract = data_extract(0, title_txt)\n",
    "\n",
    "\n",
    "    # Now extract data about the time of\n",
    "    # the submission, which will be the time\n",
    "    # for comment zero (the title)\n",
    "\n",
    "    # Extract the submission time\n",
    "    title_time_data = submission.created_utc\n",
    "\n",
    "    # Convert the extracted time from unix\n",
    "    # timetstamp form into datetime form\n",
    "    title_time_data = datetime.datetime.fromtimestamp(int(title_time_data))\n",
    "\n",
    "    # Place the comment number (zero), the extracted\n",
    "    # time and the submission title into a single\n",
    "    # data frame\n",
    "    title_time_data = pd.DataFrame(data = {'comment': [0], 'time': [title_time_data], 'text': [submission.title]})\n",
    "\n",
    "    # Finally, concatenate the date/text data\n",
    "    # for the title into the main date and text\n",
    "    # dataset\n",
    "    comment_datetxt_data = pd.concat([comment_datetxt_data, title_time_data], sort = False)\n",
    "\n",
    "\n",
    "    # Here, extract the sentiment data\n",
    "    # produced by the data extract function\n",
    "    # on title text\n",
    "    title_vanilla_sentiment = title_vanilla_extract[0]\n",
    "\n",
    "    # Append the title sentiment data to\n",
    "    # the main sentiment data set\n",
    "    sentiment_intensity_data = pd.concat([sentiment_intensity_data, title_vanilla_sentiment], sort = False)\n",
    "\n",
    "\n",
    "    # Extract the ticker counts data\n",
    "    # produced by the data extract function\n",
    "    title_vanilla_ticker_counts = title_vanilla_extract[1]\n",
    "\n",
    "    # Append the data to the main ticker counts\n",
    "    # dataset\n",
    "    ticker_counts_data = pd.concat([ticker_counts_data, title_vanilla_ticker_counts], sort = False)\n",
    "\n",
    "\n",
    "    # Here, we move on to extracting data\n",
    "    # from the comments and replies for the\n",
    "    # submission\n",
    "\n",
    "    # Create the list of comments\n",
    "    # for the submission\n",
    "    comments_list = submission.comments\n",
    "\n",
    "    # Remove redundant replies elements\n",
    "    # from the comments list\n",
    "    comments_list.replace_more(limit = 0)\n",
    "\n",
    "    # Create the comments queue, excluding the\n",
    "    # first (zeroth) comment since it's just\n",
    "    # data about the submission\n",
    "    comments_queue = comments_list[1:]\n",
    "\n",
    "    # Set the counter that will assign\n",
    "    # comments their numbers\n",
    "    j_counter = 0\n",
    "\n",
    "\n",
    "    # Loop through the comments queue until\n",
    "    # it is empty\n",
    "    while comments_queue:\n",
    "        \n",
    "        # First, extract some data\n",
    "\n",
    "        # Pop out a comment from the comment queue\n",
    "        # and set it to subject_comment, the comment\n",
    "        # analysed this iteration\n",
    "        subject_comment = comments_queue.pop(0)\n",
    "\n",
    "        # Set the comment body as the text\n",
    "        # to be analysed\n",
    "        txt = subject_comment.body\n",
    "\n",
    "        # Extract ticker and sentiment data from\n",
    "        # the comment\n",
    "        vanilla_extract = data_extract(j_counter+1, txt)\n",
    "\n",
    "        # Here we create a datetxt entry\n",
    "\n",
    "        # Extract the time of the comment\n",
    "        time_data = subject_comment.created_utc\n",
    "\n",
    "        # Change the time from unix timestamp\n",
    "        # to datetime format\n",
    "        time_data = datetime.datetime.fromtimestamp(int(time_data))\n",
    "\n",
    "        # Record the time, the comment body and the comment\n",
    "        # value into a dataframe\n",
    "        time_data = pd.DataFrame(data = {'comment': [j_counter+1], 'time': [time_data], 'text': [txt]})\n",
    "\n",
    "        # Append the time_data data frame to the\n",
    "        # main datetxt data frame\n",
    "        comment_datetxt_data = pd.concat([comment_datetxt_data, time_data], sort = False)\n",
    "\n",
    "        # Extract and recorde sentiment data\n",
    "\n",
    "        vanilla_sentiment = vanilla_extract[0]\n",
    "\n",
    "        sentiment_intensity_data = pd.concat([sentiment_intensity_data, vanilla_sentiment], sort = False)\n",
    "\n",
    "        # Extract and record ticker count data\n",
    "\n",
    "        vanilla_ticker_counts = vanilla_extract[1]\n",
    "\n",
    "        ticker_counts_data = pd.concat([ticker_counts_data, vanilla_ticker_counts], sort = False)\n",
    "\n",
    "\n",
    "        # Advance the counter by one\n",
    "        j_counter += 1\n",
    "\n",
    "        # Append the replies of the comment to the\n",
    "        # comment queue\n",
    "        comments_queue.extend(subject_comment.replies)\n",
    "\n",
    "\n",
    "    # Following code resets the index for each of\n",
    "    # three main datasets and removes the created\n",
    "    # index column\n",
    "    sentiment_intensity_data = sentiment_intensity_data.reset_index().drop(labels = 'index', axis = 1)\n",
    "\n",
    "    ticker_counts_data = ticker_counts_data.reset_index().drop(labels = 'index', axis = 1)\n",
    "\n",
    "    comment_datetxt_data = comment_datetxt_data.reset_index().drop(labels = 'index', axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "    return sentiment_intensity_data, ticker_counts_data, comment_datetxt_data\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Function for creating datasets for\n",
    "# ticker counts, comment dates, and\n",
    "# sentiment data (for submission\n",
    "# selftext)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def submission_ticker_puller(comm, submission):\n",
    "\n",
    "\n",
    "\n",
    "    sentiment_intensity_data = pd.DataFrame(data = [])\n",
    "\n",
    "    ticker_counts_data = pd.DataFrame(data = [])\n",
    "\n",
    "    comment_datetxt_data = pd.DataFrame(data = [])\n",
    "\n",
    "\n",
    "\n",
    "    self_txt = submission.selftext\n",
    "\n",
    "    self_vanilla_extract = data_extract(comm, self_txt)\n",
    "\n",
    "\n",
    "\n",
    "    self_time_data = submission.created_utc\n",
    "\n",
    "    self_time_data = datetime.datetime.fromtimestamp(int(self_time_data))\n",
    "\n",
    "    self_time_data = pd.DataFrame(data = {'submission': [comm], 'time': [self_time_data], 'text': [self_txt]})\n",
    "\n",
    "    comment_datetxt_data = pd.concat([comment_datetxt_data, self_time_data], sort = False)\n",
    "\n",
    "\n",
    "\n",
    "    self_vanilla_sentiment = self_vanilla_extract[0]\n",
    "\n",
    "    sentiment_intensity_data = pd.concat([sentiment_intensity_data, self_vanilla_sentiment], sort = False)\n",
    "\n",
    "\n",
    "\n",
    "    self_vanilla_ticker_counts = self_vanilla_extract[1]\n",
    "\n",
    "    ticker_counts_data = pd.concat([ticker_counts_data, self_vanilla_ticker_counts], sort = False)\n",
    "\n",
    "\n",
    "\n",
    "    sentiment_intensity_data = sentiment_intensity_data.reset_index().drop(labels = 'index', axis = 1)\n",
    "\n",
    "    ticker_counts_data = ticker_counts_data.reset_index().drop(labels = 'index', axis = 1)\n",
    "\n",
    "    comment_datetxt_data = comment_datetxt_data.reset_index().drop(labels = 'index', axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "    return sentiment_intensity_data, ticker_counts_data, comment_datetxt_data\n",
    "\n",
    "\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the stock ticker symbols from the NASDAQ Symbols.csv\n",
    "ticker_NASDAQ = pd.read_csv('Data Sets (Ticker Symbols)/NASDAQ Symbols.csv')\n",
    "\n",
    "# Upload the stock ticker symbols from the NYSE Symbols.csv\n",
    "ticker_NYSE = pd.read_csv('Data Sets (Ticker Symbols)/NYSE Symbols.csv')\n",
    "\n",
    "# Upload the stock ticker symbols from the AMEX Symbols.csv\n",
    "ticker_AMEX = pd.read_csv('Data Sets (Ticker Symbols)/AMEX Symbols.csv')\n",
    "\n",
    "# Create the to be set of unique stock ticker symbols\n",
    "st_tickers = set()\n",
    "\n",
    "# Update the st_tickers set with ticker codes from\n",
    "# NASDAQ, NYSE and AMEX\n",
    "st_tickers.update(set(ticker_NASDAQ['Symbol']))\n",
    "st_tickers.update(set(ticker_NYSE['Symbol']))\n",
    "st_tickers.update(set(ticker_AMEX['Symbol']))\n",
    "\n",
    "# Code for determining if the ticker code is in\n",
    "# the st_tickers set\n",
    "# 'A' in st_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for excluding single letter ticker symbols\n",
    "\n",
    "# Create list of ticker symbols to exclude\n",
    "exclusion_list = [i for i in st_tickers if len(i) == 1]\n",
    "\n",
    "# Modify st_tickers to exclude the unneeded symbols\n",
    "st_tickers.difference_update(exclusion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a subreddit\n",
    "\n",
    "# Examples of stock market related subreddits\n",
    "\n",
    "# 'wallstreetbets'\n",
    "# 'stocks'\n",
    "# 'investing'\n",
    "# 'pennystocks'\n",
    "# 'RobinHood'\n",
    "\n",
    "# Set the subreddit to analyse\n",
    "subreddit = reddit.subreddit('wallstreetbets')\n",
    "\n",
    "# Print the subreddit's name\n",
    "print(subreddit.display_name)\n",
    "\n",
    "# Print the subreddit's title\n",
    "# print(subreddit.title)\n",
    "\n",
    "# Print the subreddit's description\n",
    "# print(subreddit.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list object into which submissions are placed\n",
    "submission_list = []\n",
    "\n",
    "# Cycle through submissions in hot category\n",
    "# limit parameter states number of submissions to record\n",
    "# In case of none, there is no limit to how many to record\n",
    "# So all are recorded\n",
    "for submission in subreddit.hot(limit = None):\n",
    "    \n",
    "    submission_list.append(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(submission_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose submission to analyse\n",
    "i = 0\n",
    "\n",
    "# Set the submission\n",
    "submission = submission_list[i]\n",
    "\n",
    "# Details about submission\n",
    "print(submission.title)\n",
    "print()\n",
    "print(submission.selftext)\n",
    "print()\n",
    "print(submission.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for printing the body text of\n",
    "# all comments and replies of a submission\n",
    "\n",
    "# Create a list of comments\n",
    "comments_list = submission.comments\n",
    "\n",
    "# Separates replies from list of comments\n",
    "comments_list.replace_more(limit = 0)\n",
    "\n",
    "# List of comments without replies\n",
    "# print(comments_list.list())\n",
    "\n",
    "# Transform the comment list into an\n",
    "# actual list object\n",
    "comments_queue = comments_list[1:]\n",
    "\n",
    "# Cycle through all comments in list\n",
    "while comments_queue:\n",
    "    \n",
    "    # Set the printed comment as the one popped\n",
    "    # from the comment queue, so it's read and\n",
    "    # removed from the queue\n",
    "    comment = comments_queue.pop(0)\n",
    "    \n",
    "    # Print the comment's body\n",
    "    print(comment.body)\n",
    "    print()\n",
    "    \n",
    "    # Other information about the comment is\n",
    "    # readily avaliable in this window here\n",
    "    \n",
    "    # Extend the comment queue with the replies\n",
    "    # of the comment printed, then move on to the\n",
    "    # next comment\n",
    "    comments_queue.extend(comment.replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentiment_dataset = pd.DataFrame(data = [])\n",
    "\n",
    "final_ticker_dataset = pd.DataFrame(data = [])\n",
    "\n",
    "final_datetxt_dataset = pd.DataFrame(data = [])\n",
    "\n",
    "\n",
    "\n",
    "submission_queue = submission_list\n",
    "\n",
    "subm = 0\n",
    "\n",
    "\n",
    "\n",
    "while submission_queue:\n",
    "\n",
    "    submission = submission_queue.pop(0)\n",
    "\n",
    "    subm_unitdata = pd.DataFrame(data = [subm], columns = ['submission'])\n",
    "\n",
    "\n",
    "\n",
    "    final_data = ticker_puller(submission)\n",
    "\n",
    "\n",
    "\n",
    "    subm_data_0 = pd.concat([subm_unitdata, final_data[0]], sort = False, axis = 1)\n",
    "\n",
    "    subm_data_0['submission'] = subm_data_0['submission'].fillna(subm)\n",
    "\n",
    "\n",
    "\n",
    "    subm_data_1 = pd.concat([subm_unitdata, final_data[1]], sort = False, axis = 1)\n",
    "\n",
    "    subm_data_1['submission'] = subm_data_1['submission'].fillna(subm)\n",
    "\n",
    "\n",
    "\n",
    "    subm_data_2 = pd.concat([subm_unitdata, final_data[2]], sort = False, axis = 1)\n",
    "\n",
    "    subm_data_2['submission'] = subm_data_2['submission'].fillna(subm)\n",
    "\n",
    "\n",
    "\n",
    "    final_sentiment_dataset = pd.concat([final_sentiment_dataset, subm_data_0], sort = False)\n",
    "\n",
    "    final_ticker_dataset = pd.concat([final_ticker_dataset, subm_data_1], sort = False)\n",
    "\n",
    "    final_datetxt_dataset = pd.concat([final_datetxt_dataset, subm_data_2], sort = False)\n",
    "\n",
    "\n",
    "\n",
    "    final_sentiment_dataset = final_sentiment_dataset.reset_index().drop(labels = 'index', axis = 1)\n",
    "\n",
    "    final_ticker_dataset = final_ticker_dataset.reset_index().drop(labels = 'index', axis = 1)\n",
    "\n",
    "    final_datetxt_dataset = final_datetxt_dataset.reset_index().drop(labels = 'index', axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "    print('Submission', str(subm), 'is added')\n",
    "\n",
    "    subm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_sentiment_dataset.to_csv('data_sets_usable/final_sentiment_dataset.csv')\n",
    "\n",
    "# final_ticker_dataset.to_csv('data_sets_usable/final_ticker_dataset.csv')\n",
    "\n",
    "# final_datetxt_dataset.to_csv('data_sets_usable/final_datetxt_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
