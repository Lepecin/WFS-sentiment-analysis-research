{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WFS Sentiment Analysis Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This research report studies the effectiveness of using sentiment and stock ticker mentions data from Reddit posts to explain price and volume changes in the stock market.\n",
    "\n",
    "The project behind this report utilised the Python programming language to extract and analyse the necessary data.\n",
    "The main tasks of the project involved:\n",
    "- Studying text data, comprising user submissions and comments, coming from the hot section of the r/wallstreebets subreddit.\n",
    "- Carrying out sentiment analysis with a natural language processing (NLP) method that assigns sentiment scores to text data.\n",
    "- Comparing the finalised data to stock price and volume data extracted from Yahoo Finance.\n",
    "\n",
    "The motivation for this report is to further justify using big data to aid in predicting market changes. The research can serve to inspire others to create trading algorithms based on human behavioural data. The report will also increase WFS’s involvement with programming and data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scientific Preliminaries\n",
    "\n",
    "The project involved testing the hypothesis below:\n",
    "\n",
    "*Can one infer future stock market changes using sentiment intensity and stock ticker mentions data from Reddit posts?*\n",
    "\n",
    "Before proceeding straight to evaluating the evidence for the hypothesis, it’s essential to address what evidence (data sets) was sourced and answer the following questions for each of them:\n",
    "- From where was the data sourced?\n",
    "- How was the data sourced?\n",
    "- How reliable is the data regarding what it represents?\n",
    "\n",
    "We also consider the form of the data, which determines the ease with which one can use the data as evidence. Lastly, we state how the data was used to gain inference for the project’s hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Data\n",
    "\n",
    "The required data is presented in the following two tables. \n",
    "\n",
    "The first table lists the raw data, data extracted directly from the internet.\n",
    "\n",
    "| **Raw Data** | **Description** |\n",
    "| --- | --- |\n",
    "| Text Data | The body text of each subreddit submission and comment |\n",
    "| Time Data | The date and time of publication of each submission and comment |\n",
    "| Stock Price Data | The price time series of the stocks analysed |\n",
    "| Volume Data | The volume time series of the stocks analysed |\n",
    "| Ticker List | The list of current stock ticker symbols in the stock market |\n",
    "\n",
    "The second table lists the processed data, usable data yielded from the raw data.\n",
    "\n",
    "| **Processed Data** | **Description** |\n",
    "| --- | --- |\n",
    "| Sentiment Data | The sentiment intensity of each entry in the text data expressed numerically |\n",
    "| Ticker Mentions | The tickers mentioned in each entry in the text data |\n",
    "| Ticker Frequency | The number of tickers found for each text data entry and ticker |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text & Time Data\n",
    "\n",
    "The r/wallstreetbets subreddit was chosen the source of text data for the project because of its history of significantly affecting stock market prices.\n",
    "\n",
    "At around January 2021, r/wallstreetbets users collaborated on forcefully increasing GME’s (GameStop’s) stock price through excessive short-selling of the stock. The effort drove several companies and hedge funds into losses. This event served as a reminder that social media activity can significantly affect the real world, including the market.\n",
    "\n",
    "The PRAW (Python Reddit API Wrapper) Python package was used to pull text data from r/wallstreetbets. This package is usable in conjunction with the PSAW (Python Pushshift.io API Wrapper) to attain greater flexibility in choosing what submission types we pull from the subreddit, including specific dates. The wrappers also provide data on the postage time of each submission and comment.\n",
    "\n",
    "Subreddits offer a few categories to view submissions from, which are:\n",
    "- Hot, containing submissions with overwhelmingly fast upvote rates\n",
    "- New, containing the most recent subreddit submissions\n",
    "- Rising, containing new submissions with high upvote rates\n",
    "- Top, containing submissions that accumulated the most number of upvotes for a given period\n",
    "\n",
    "The hot section of r/wallstreetbets was chosen as our main source, as it’s the first page any user sees upon entering the subreddit and likely interacts with, and it has the most number of comments overall. Moreover, sourcing from the hot section ensured that data was pulled from human approved submissions and comments not written by spambots.\n",
    "\n",
    "Every subreddit submission has a posting time given to the nearest second. Thus, one can analyse subreddit text data at minute intervals if users publish submissions at a high enough rate. Hot submissions from r/wallstreetbets have a posting frequency of about three per hour, which Reddit automatically replaces when they stop trending or after some time. Therefore, it was enough to analyse the text data on a day to day basis for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Price, Volume & Ticker Data\n",
    "\n",
    "The stock price and volume data was sourced from Yahoo Finance. Investors widely regard Yahoo Finance as a reliable source of stock data. Yahoo provides the data free of charge for any desired period and for almost any time interval between each price update (to the nearest minute).\n",
    "\n",
    "The stock market prices and volume data from Yahoo Finance were extracted using the yfinance Python package. \n",
    "\n",
    "NYSE, AMEX and NASDAQ ticker symbol lists were downloaded from the official NASDAQ website. It's entirly possible to use tickers from other markets and index tickers too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Data\n",
    "\n",
    "The NLTK (Natural Language Toolkit) Python package was used to create sentiment intensity scores for the text data. NLTK is a highly respected natural language processing package providing a broad assortment of NLP tools.\n",
    "\n",
    "The package provides the “SentimentIntensityAnalyzer()” function class for assigning sentiment intensity scores to Python strings. These scores are:\n",
    "- ‘pos’, the proportion of the text string consisting of positive sentiment\n",
    "- ‘neu’, the proportion of the text string consisting of neutral sentiment (no sentiment)\n",
    "- ‘neg’, the proportion of the text string consisting of negative sentiment\n",
    "- ‘compound’, a normalised sum of the sentiment scores of each component of the given string (note that NLTK uses neither ‘neg’, ‘neu’ nor ‘pos’ for calculating ‘compound’)\n",
    "\n",
    "Analysts often use the compound score (statistic) to measure sentiment in political comments to determine the proportion of internet users who favour a given political party. Similarly, one can evaluate the sentiment towards different stocks by assigning a text’s compound score to any stock ticker symbol found in it. It's possibke to utilise the pos, neg and neu scores for the analysis as well, which were omitted for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticker Mentions & Frequency\n",
    "\n",
    "To determine what ticker symbols a given text mentions, wea script was created that removes all characters from a text string that do not compose any ticker symbols (e.g. lower case letters). Then, the script records the tickers by comparing the leftover text with the ticker list, detecting which tickers are present and how many.\n",
    "\n",
    "The r/wallstreetbets subreddit is prone to spambots that can potentially promote specific stocks with spam. The Python script for recording ticker symbols is vulnerable to such artificial mentions and could lead to false signals. This vulnerability further justified sourcing text data from the hot category of the subreddit, as bot submissions don’t usually reach the hot section.\n",
    "\n",
    "A technical issue arising from the ticker recording script is that it also pulled single character ticker symbols, even when the analysed text did not explicitly mention them. Any sentences in the text data that started with capital letters contributed to the single letter ticker frequencies. Thus, a filter was implemented to the script, so it ignores any single character ticker symbols it finds. The ability to detect single letter tickers was sacrificed to remove excessive noise from the data.\n",
    "\n",
    "Another criticism is that the script does not detect any mentions of the company names themselves. Not all users refer to stocks by their tickers. Implementing a script for detecting company names may require a more sophisticated approach, as company names are not usually mentioned by their full names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format\n",
    "\n",
    "Four data sets were constructed from the data collected, the third one being specific to a given ticker symbol. Each dataset was designed to minimise the information loss upon transforming the data when using it as evindence. These data sets have the following features:\n",
    "\n",
    "**Comment/Sentiment Data Set**\n",
    "\n",
    "| **Feature** | **Basis Data** | **Description** |\n",
    "| --- | --- | --- |\n",
    "| ‘submission’ | Integer Label | The feature for uniquely associating each observation with a submission number |\n",
    "| ‘comment’ | Integer Label | The feature for uniquely associating each observation with a comment number for a given submission number (comment zero identifies the title of the submission for each submission number) |\n",
    "| ‘time’ | Time Data | The feature containing the publication times of each comment |\n",
    "| ‘text’ | Text Data | The feature containing the body text of each comment |\n",
    "| ‘compound’ | Sentiment Data | The feature containing the compounded sentiment scores for each comment |\n",
    "\n",
    "Each 'text' observation has a unique 'submission' and 'comment' label pair. Hence, the 'submission' and 'comment' features collectively act as the parent key (ID variable) for this data set and the following ticker data set.\n",
    "\n",
    "**Ticker Data Set**\n",
    "\n",
    "| **Feature** | **Basis Data** | **Description** |\n",
    "| --- | --- | --- |\n",
    "| ‘submission’ | Integer Label | Same as before |\n",
    "| ‘comment’ | Integer Label | Same as before |\n",
    "| ‘ticker’ | Ticker List & Mentions | The feature containing the ticker symbols mentioned for each comment |\n",
    "| ‘count’ | Ticker Frequency | The feature containing the frequencies of tickers for each ticker and comment |\n",
    "\n",
    "The comment/sentiment and the ticker data sets were separated, despite them having the same parent key(s), to avoid creating observations with missing data in the comment/sentiment data set. The ticker data set does not have data for some combinations of 'submission' and 'comment'.\n",
    "\n",
    "**Stock Price & Volume (Market) Data Set (for a given stock)**\n",
    "\n",
    "| **Feature** | **Basis Data** | **Description** |\n",
    "| --- | --- | --- |\n",
    "| ‘time’ | Stock Price/Volume Data | The feature containing the times or dates for each observation |\n",
    "| ‘open’ | Stock Price Data | The feature containing the opening price of the subject stock for a given time/date |\n",
    "| ‘close’ | Stock Price Data | The feature containing the closing price of the subject stock for a given time/date |\n",
    "| ‘volume’ | Volume Data | The feature containing the trade volume for the subject stock for a given time/date |\n",
    "\n",
    "A total of five market data sets were used in this project corresponding to the top five mentioned (popular) tickers in the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utilisation\n",
    "\n",
    "The overall popularity of each ticker symbol was determined by counting the number of times each symbol appears in a comment. Note that this method did not require the ticker frequency data. Below is the table of the top ten mentioned tickers from the 22nd November 2021 to 3rd December 2021:\n",
    "\n",
    "| **Ticker** | **Company** | **Mentions** |\n",
    "| --- | --- | --- |\n",
    "| GME | GameStop | 555 |\n",
    "| WISH | Wish | 474 |\n",
    "| DD | DuPont | 312 |\n",
    "| TSLA | Tesla | 252 |\n",
    "| PLTR | Palantir Technologies | 220 |\n",
    "| BABA | Alibaba Group | 217 |\n",
    "| AMD | Advanced Micro Devices | 205 |\n",
    "| BB | BlackBerry | 204 |\n",
    "| CLOV | Clover Health | 193 |\n",
    "| AMC | AMC Entertainment | 166 |\n",
    "\n",
    "Observing the table, we see that the five most popular ticker symbols throughout the twelve days were GME, WISH, DD, TSLA and PLTR. For each of the top five ticker symbols, time series of the frequencies and sentiment scores at daily intervals were created by joining the ticker dataset with the comment/sentiment dataset.\n",
    "\n",
    "Since each ticker has two time series representing data from Reddit (ticker frequencies and sentiment) and three representing its market data (open, close and volume), a total of six data set comparisons were made for each symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GME Graphs\n",
    "\n",
    "![](ticker_graphs/GME_graphs.png)\n",
    "\n",
    "## WISH Graphs\n",
    "\n",
    "![](ticker_graphs/WISH_graphs.png)\n",
    "\n",
    "## DD Graphs\n",
    "\n",
    "![](ticker_graphs/DD_graphs.png)\n",
    "\n",
    "## TSLA Graphs\n",
    "\n",
    "![](ticker_graphs/TSLA_graphs.png)\n",
    "\n",
    "## PLTR Graphs\n",
    "\n",
    "![](ticker_graphs/PLTR_graphs.png)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
